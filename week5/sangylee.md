# 카프카 완벽 가이드: 내부 구조와 핵심 메커니즘
카프카의 내부 구조를 이해하는 것은 실제 운영이나 애플리케이션 개발에 필수적이지는 않지만, 카프카의 동작 방식과 문제 해결에 중요한 컨텍스트를 제공합니다. 이 내용에서는 카프카 실무자에게 특히 중요한 몇 가지 주제를 살펴보겠습니다.

## 클러스터 멤버십

카프카는 Apache ZooKeeper를 사용하여 현재 클러스터에 속한 브로커 목록을 관리합니다. 모든 브로커는 고유 식별자를 가지며, 이는 브로커 구성 파일에서 설정되거나 자동으로 생성됩니다. 브로커 프로세스가 시작될 때마다 ZooKeeper에 임시(ephemeral) 노드를 생성하여 자신의 ID로 등록합니다.

카프카 브로커, 컨트롤러 및 일부 에코시스템 도구는 브로커가 등록된 ZooKeeper의 `/brokers/ids` 경로를 구독하여 브로커가 추가되거나 제거될 때 알림을 받습니다. 동일한 ID를 가진 다른 브로커를 시작하려고 하면 오류가 발생합니다. 새 브로커가 등록을 시도하지만 이미 동일한 브로커 ID에 대한 ZooKeeper 노드가 있기 때문에 실패합니다

브로커가 ZooKeeper에 대한 연결을 잃으면(보통 브로커가 중지되거나 네트워크 파티션 또는 긴 가비지 컬렉션 일시 중지로 인해 발생), 브로커가 시작할 때 생성한 임시 노드가 ZooKeeper에서 자동으로 제거됩니다. 브로커 목록을 모니터링하는 카프카 구성 요소는 해당 브로커가 사라졌다는 알림을 받게 됩니다

브로커가 중지되면 해당 노드는 사라지지만, 브로커 ID는 여전히 다른 데이터 구조에 존재합니다. 예를 들어, 각 토픽의 복제본 목록에는 복제본에 대한 브로커 ID가 포함되어 있습니다. 이렇게 하면 브로커를 완전히 잃고 이전 브로커의 ID로 새 브로커를 시작하더라도, 새 브로커는 즉시 누락된 브로커를 대신하여 클러스터에 참여하고 동일한 파티션과 토픽이 할당됩니다

## 컨트롤러 역할과 선출 과정

컨트롤러는 일반적인 브로커 기능 외에도 파티션 리더를 선출하는 역할을 담당하는 카프카 브로커입니다. 클러스터에서 가장 먼저 시작된 브로커는 ZooKeeper에 `/controller`라는 임시 노드를 생성하여 컨트롤러가 됩니다. 다른 브로커가 시작되면 이 노드를 생성하려고 시도하지만 "노드가 이미 존재함" 예외를 받아 클러스터에 이미 컨트롤러가 있다는 것을 인식합니다

브로커들은 컨트롤러 노드에 ZooKeeper 감시(watch)를 설정하여 이 노드의 변경 사항을 알림 받습니다. 이렇게 하면 클러스터에 한 번에 하나의 컨트롤러만 존재하도록 보장됩니다[

컨트롤러 브로커가 중지되거나 ZooKeeper에 대한 연결이 끊어지면 임시 노드가 사라집니다. 이는 컨트롤러가 ZooKeeper에 `zookeeper.session.timeout.ms` 이상 하트비트를 보내지 않는 경우도 포함합니다. 임시 노드가 사라지면 클러스터의 다른 브로커들은 ZooKeeper 감시를 통해 컨트롤러가 사라졌다는 알림을 받고 ZooKeeper에 컨트롤러 노드를 직접 생성하려고 시도합니다

ZooKeeper에 새 컨트롤러를 처음으로 생성하는 노드가 다음 컨트롤러가 되며, 다른 노드는 "노드가 이미 존재함" 예외를 받고 새 컨트롤러 노드에 대한 감시를 다시 설정합니다. 컨트롤러가 선출될 때마다 ZooKeeper 조건부 증가 연산을 통해 새롭고 더 높은 컨트롤러 에포크(epoch) 번호를 받습니다

브로커들은 현재 컨트롤러 에포크를 알고 있으며, 이전 번호를 가진 컨트롤러로부터 메시지를 받으면 이를 무시합니다. 이는 컨트롤러 브로커가 긴 가비지 컬렉션 일시 중지로 인해 ZooKeeper에서 연결이 끊어질 때 중요합니다. 이 일시 중지 동안 새 컨트롤러가 선출되고, 이전 리더가 일시 중지 후 작업을 재개하면 새 컨트롤러가 있다는 것을 알지 못한 채 브로커에 메시지를 계속 보낼 수 있습니다

컨트롤러가 처음 시작될 때, 클러스터 메타데이터를 관리하고 리더 선출을 수행하기 전에 ZooKeeper에서 최신 복제본 상태 맵을 읽어야 합니다. 로딩 프로세스는 비동기 API를 사용하고 ZooKeeper에 대한 읽기 요청을 파이프라인으로 처리하여 지연 시간을 숨깁니다. 그러나 많은 파티션이 있는 클러스터에서는 로딩 프로세스가 몇 초 걸릴 수 있습니다

## KRaft: 카프카의 새로운 Raft 기반 컨트롤러

2019년부터 Apache 카프카 커뮤니티는 ZooKeeper 기반 컨트롤러에서 Raft 기반 컨트롤러 쿼럼으로 전환하는 프로젝트를 시작했습니다. KRaft라는 새로운 컨트롤러의 미리보기 버전은 Apache 카프카 2.8 릴리스의 일부입니다. 2021년 중반에 계획된 Apache 카프카 3.0 릴리스에는 KRaft의 첫 번째 프로덕션 버전이 포함될 예정이며, 카프카 클러스터는 기존 ZooKeeper 기반 컨트롤러나 KRaft로 실행할 수 있게 됩니다

카프카 커뮤니티가 컨트롤러를 교체하기로 결정한 이유는 무엇일까요? 카프카의 기존 컨트롤러는 이미 여러 번 재작성되었지만, 토픽, 파티션 및 복제본 정보를 저장하기 위해 ZooKeeper를 사용하는 방식이 개선되었음에도 불구하고, 기존 모델이 카프카가 지원하려는 파티션 수까지 확장되지 않을 것이 분명해졌습니다

변경을 촉진한 몇 가지 알려진 문제:

1. 메타데이터 업데이트는 ZooKeeper에 동기적으로 기록되지만 브로커에는 비동기적으로 전송됩니다. 또한 ZooKeeper에서 업데이트를 수신하는 것도 비동기적입니다. 이로 인해 브로커, 컨트롤러 및 ZooKeeper 간에 메타데이터가 불일치하는 엣지 케이스가 발생합니다

2. 컨트롤러가 재시작될 때마다 ZooKeeper에서 모든 브로커와 파티션에 대한 모든 메타데이터를 읽은 다음 이 메타데이터를 모든 브로커에 보내야 합니다. 수년간의 노력에도 불구하고 이것은 여전히 큰 병목 현상입니다

3. 메타데이터 소유권에 관한 내부 아키텍처가 좋지 않습니다. 일부 작업은 컨트롤러를 통해, 다른 작업은 브로커를 통해, 또 다른 작업은 ZooKeeper에 직접 수행되었습니다

4. ZooKeeper는 자체적인 분산 시스템이며, 카프카와 마찬가지로 운영에 약간의 전문 지식이 필요합니다. 카프카를 사용하려는 개발자는 두 개의 분산 시스템을 학습해야 합니다

새 컨트롤러 설계의 핵심 아이디어는 카프카 자체가 로그 기반 아키텍처를 가지고 있다는 것이며, 사용자가 상태를 이벤트 스트림으로 표현합니다. 새 컨트롤러 아키텍처는 카프카의 메타데이터 관리에 동일한 이점을 제공합니다

새로운 아키텍처에서 컨트롤러 노드는 메타데이터 이벤트 로그를 관리하는 Raft 쿼럼입니다. 이 로그에는 클러스터 메타데이터의 각 변경 사항에 대한 정보가 포함되어 있습니다. 현재 ZooKeeper에 저장된 토픽, 파티션, ISR, 구성 등 모든 정보가 이 로그에 저장됩니다

Raft 알고리즘을 사용하여 컨트롤러 노드는 외부 시스템에 의존하지 않고 자체적으로 리더를 선출합니다. 메타데이터 로그의 리더는 액티브 컨트롤러라고 불립니다. 액티브 컨트롤러는 브로커에서 만든 모든 RPC를 처리합니다. 팔로워 컨트롤러는 액티브 컨트롤러에 기록된 데이터를 복제하고 액티브 컨트롤러에 오류가 발생할 경우 대기 상태로 작동합니다

## 복제 메커니즘

복제는 카프카 아키텍처의 핵심입니다. 카프카는 종종 "분산된, 파티션된, 복제된 커밋 로그 서비스"로 설명됩니다. 복제는 개별 노드가 불가피하게 실패할 때 카프카가 가용성과 내구성을 보장하는 방법이기 때문에 중요합니다

이미 논의한 바와 같이 카프카의 데이터는 토픽별로 구성됩니다. 각 토픽은 파티션으로 나뉘며, 각 파티션은 여러 복제본을 가질 수 있습니다. 이러한 복제본은 브로커에 저장되며, 각 브로커는 일반적으로 다른 토픽 및 파티션에 속하는 수백 또는 수천 개의 복제본을 저장합니다

두 가지 유형의 복제본이 있습니다:

1. **리더 복제본**: 각 파티션에는 리더로 지정된 단일 복제본이 있습니다. 일관성을 보장하기 위해 모든 생산 요청은 리더를 통해 이루어집니다. 클라이언트는 리더 복제본이나 팔로워 복제본에서 소비할 수 있습니다

2. **팔로워 복제본**: 리더가 아닌 파티션의 모든 복제본을 팔로워라고 합니다. 다르게 구성되지 않는 한, 팔로워는 클라이언트 요청을 처리하지 않습니다. 그들의 주요 작업은 리더로부터 메시지를 복제하고 리더가 가진 최신 메시지와 동기화를 유지하는 것입니다. 파티션의 리더 복제본이 충돌하면 팔로워 복제본 중 하나가 승격되어 파티션의 새 리더가 됩니다

리더의 또 다른 책임은 어떤 팔로워 복제본이 리더와 동기화되어 있는지 아는 것입니다. 팔로워는 메시지가 도착할 때 리더의 모든 메시지를 복제하여 동기화를 유지하려고 시도하지만, 네트워크 혼잡이 복제를 늦추거나 브로커가 충돌하여 해당 브로커의 모든 복제본이 브로커를 시작하고 복제를 다시 시작할 때까지 계속 뒤쳐지는 등 다양한 이유로 동기화 상태를 유지하지 못할 수 있습니다

리더와 동기화를 유지하기 위해 복제본은 리더에게 Fetch 요청을 보냅니다. 이는 소비자가 메시지를 소비하기 위해 보내는 것과 정확히 같은 유형의 요청입니다. 이러한 요청에 대한 응답으로 리더는 복제본에 메시지를 보냅니다. 이러한 Fetch 요청에는 복제본이 다음에 받기를 원하는 메시지의 오프셋이 포함되어 있으며, 항상 순서대로 진행됩니다

이는 리더가 복제본이 복제본이 가져온 마지막 메시지까지의 모든 메시지를 받았고, 그 이후의 메시지는 받지 않았음을 알 수 있다는 것을 의미합니다. 각 복제본이 요청한 마지막 오프셋을 보면 리더는 각 복제본이 얼마나 뒤쳐져 있는지 알 수 있습니다. 복제본이 10초 이상 메시지를 요청하지 않았거나, 메시지를 요청했지만 10초 이상 최신 메시지를 따라잡지 못한 경우 해당 복제본은 동기화되지 않은 것으로 간주됩니다. 복제본이 리더를 따라잡지 못하면 실패 시 새 리더가 될 수 없습니다. 결국, 모든 메시지를 포함하고 있지 않기 때문입니다

이와 반대로, 지속적으로 최신 메시지를 요청하는 복제본을 동기화된 복제본(in-sync replicas)이라고 합니다. 기존 리더가 실패할 경우 파티션 리더로 선출될 자격이 있는 것은 동기화된 복제본뿐입니다

팔로워가 비활성 상태이거나 동기화되지 않은 것으로 간주되기 전에 뒤쳐질 수 있는 시간은 `replica.lag.time.max.ms` 구성 매개변수로 제어됩니다. 이 허용된 지연은 리더 선출 중 클라이언트 동작 및 데이터 보존에 영향을 미칩니다

## 결론

카프카의 내부 구조를 이해하는 것은 카프카를 효과적으로 운영하고 문제 해결을 위해 중요합니다. 클러스터 멤버십, 컨트롤러 역할, KRaft와 같은 새로운 발전, 그리고 복제 메커니즘은 카프카의 핵심 내부 메커니즘입니다. 이러한 구성 요소들이 어떻게 작동하는지 이해함으로써 카프카 시스템을 더 잘 튜닝하고 관리할 수 있습니다.
