(... Replication까지의 내용 ...)

# 1. 클러스터와 컨트롤러

## ZooKeeper 기반 클러스터 구성

Kafka는 분산 메시지 시스템으로, 여러 개의 브로커들이 함께 클러스터를 구성해서 데이터를 처리한다.  
그런데 이 브로커들이 언제 살아있고, 언제 죽었는지를 Kafka가 알 수 있게 하는 것이 바로 **ZooKeeper**이다.  
ZooKeeper는 Kafka 클러스터의 ‘감시자’ 같은 역할이라고 생각할 수 있다.

Kafka에서는 ZooKeeper를 통해 브로커의 생사 여부, 클러스터 멤버십, 컨트롤러 선출 등을 처리한다.  
즉, 브로커들의 상태를 관리하기 위해 ZooKeeper를 사용하는 것이다.

### 브로커 시작 시 ZooKeeper와의 상호작용

1. 각 Kafka 브로커는 고유한 ID를 부여받고, 보통 설정 파일에서 지정되거나 자동 생성된다.
2. 브로커는 실행될 때 ZooKeeper에 자신을 **ephemeral node**로 등록한다.  
   이는 브로커가 죽거나 연결이 끊기면 자동으로 제거된다.  
   → 브로커가 ‘나 살아있어요’ 하고 등록하는 셈이다.
3. 다른 구성요소는 이 경로(`/brokers/ids`)를 감시하고 있다.  
   Kafka에서 다른 브로커들이나 컨트롤러는 누가 등록되고 사라지는지를 실시간 감시할 수 있다.

📌 브로커가 강제 종료되거나 ZooKeeper와 연결이 끊기면 해당 ephemeral node는 자동 삭제된다.  
📌 브로커가 다시 특정 ID로 재시작하면, 이전에 담당하던 파티션과 토픽 정보를 재활용하여 클러스터에 빠르게 복귀한다.


## Kafka에서 컨트롤러의 역할

Kafka는 여러 대의 브로커가 함께 동작하는 **분산 시스템**이다.  
그렇기 때문에 누군가는 **지휘자** 역할을 맡아 전체 브로커들을 조율해줘야 하는데, 이 지휘자 역할을 하는 것이 바로 Kafka의 컨트롤러다.

- Kafka 클러스터에는 항상 **하나의 컨트롤러**만 존재한다.
- 컨트롤러는 **파티션의 리더 선출**, **클러스터 메타데이터 관리** 역할을 한다.
- ZooKeeper의 `/controller` 노드를 통해 선출되며, 이 노드가 사라지면 다른 브로커가 다시 컨트롤러로 선출된다.
- 특정 브로커가 다운되면, 해당 브로커가 리더였던 파티션의 **새 리더를 선출**하고, 메타데이터 갱신 정보를 모든 브로커에게 전달한다.

### 왜 컨트롤러가 필요할까?

Kafka에서는 각 파티션에 대해 하나의 리더를 정해야 한다.  
프로듀서와 컨슈머는 항상 리더와 통신해야만 메시지를 주고받을 수 있기 때문이다.

그런데 리더 브로커가 꺼지면 해당 파티션은 더 이상 클라이언트 요청을 처리할 수 없다.  
→ 이때 **컨트롤러가 개입**하여 새로운 리더를 선출하는 것이다.


### 👓 컨트롤러는 어떻게 선출되는가?

1. 브로커들이 동시에 시작되면, 각자 ZooKeeper의 `/controller` 노드를 생성하려 한다.  
   (이 노드는 **선착순 한 명만 만들 수 있는 일회용 좌석** 같은 것)
2. 가장 먼저 만든 브로커가 **컨트롤러가 된다.**
3. 나머지 브로커들은 **"아~ 이미 컨트롤러 있구나"** 하고 감시만 한다.



### 👓 컨트롤러가 꺼지면?

1. ZooKeeper는 컨트롤러와의 연결이 끊기면 `/controller` 노드를 자동 삭제한다.
2. 다른 브로커들은 이 노드를 감시하고 있다가, **컨트롤러 사라짐**을 감지한다.
3. 다시 누가 컨트롤러가 될지 **경쟁**을 시작한다.
4. 선착순으로 노드를 만든 브로커가 **새 컨트롤러**가 된다.



### 👓 컨트롤러가 일시적으로 멈췄다가 다시 살아나는 경우?

→ 중복 컨트롤러 발생 가능성이 있다. 이를 방지하기 위해 Kafka는 **컨트롤러 epoch 번호**를 사용한다.

- 브로커가 메시지를 받을 때 epoch 번호를 확인한다.
- 더 오래된 epoch 번호로부터 받은 메시지는 **무시**한다.
- → 즉, **더 최근에 생성된 컨트롤러만 유효**하게 작동한다.


## KRaft (Kafka Raft)

Kafka 2.8 이후부터는 ZooKeeper 없이 동작 가능한 구조로 전환되고 있으며, 이를 **KRaft**라고 부른다.

기존 Kafka는 ZooKeeper를 따로 운영해야 하고,  
컨트롤러가 바뀌면 ZooKeeper에서 메타데이터를 다시 읽어야 하므로 느리고 복잡하다.  
Kafka 하나만 알아서는 운영이 어렵고, 이중 구성의 부담도 있다.

→ 그래서 Kafka는 **"ZooKeeper 없이도 돌아가야 하지 않을까?"** 하는 방향으로 나아가고 있다.



## Raft 알고리즘이란?

**분산 시스템에서 여러 서버가 하나의 리더를 뽑고, 변경사항을 합의하는 방식**이다.

- 누가 리더가 될지, 어떤 변경을 수용할지를 **다수결로 결정**한다.
- Kafka는 이 Raft 알고리즘을 사용해, **메타데이터를 Kafka 내부 로그로 관리**한다.



### Raft에서 컨트롤러는 어떻게 선출되는가?

Raft는 총 3가지 상태 중 하나를 가진다:

- **Follower**: 기본 상태
- **Candidate**: 리더가 없을 때 자천
- **Leader**: 투표에서 다수득표한 노드

### 선출 과정

1. Follower들은 일정 시간 동안 리더의 heartbeat를 못 받으면 **Candidate**가 된다.
2. Candidate는 **자기 자신을 리더로 뽑아달라**고 투표 요청을 보낸다.
3. 다른 노드(Follower)는 **조건에 따라 단 한 번만 투표**한다.
4. 다수의 투표를 얻은 후보가 **Leader**가 된다.

단, 아무에게나 투표하지는 않는다.

- 후보가 제안한 **메타데이터 로그가 더 최신**일 경우에만 투표한다.
- 이미 투표한 term에서는 **다시 투표하지 않는다**.

---

# 2. 복제 (Replication)

복제는 Kafka의 **고가용성(HA)**과 **데이터 내구성**을 보장하는 핵심 메커니즘이다.

Kafka는 데이터를 유실 없이 빠르게 처리하기 위해 **Replication 구조**를 가진다.


## 리더와 팔로워가 무엇인가?

Kafka는 데이터를 **파티션(partition)** 단위로 나눠서 저장한다.  
예: A라는 토픽 → 3개의 파티션 → 여러 Kafka 브로커에 분산

📌 단, **하나의 브로커에만 저장하면** 브로커 장애 시 데이터 유실 가능  
→ Kafka는 **파티션을 복제**한다.

각 파티션은 다음 두 가지 역할을 가진다:

- **Leader**: 모든 읽기/쓰기 요청을 담당
- **Follower**: 리더의 데이터를 실시간으로 복제

즉, Kafka는 **Leader-Follower 구조**를 따른다.  
리더가 죽으면, 팔로워 중 하나가 **리더가 될 수 있다.**  
→ Kafka는 이렇게 **고가용성**을 보장한다.


## ISR (In-Sync Replica)

**리더와 거의 실시간으로 동기화된 팔로워**를 ISR이라고 부른다.

- 리더가 장애나면 ISR 중 **하나만** 리더로 승격된다.
- ISR 유지 여부는 `replica.lag.time.max.ms` 설정으로 판단한다.

📌 이 시간 이상 지연되면 ISR에서 **제외**된다.  
📌 ISR이 아닌 팔로워는 **리더가 될 수 없다** (데이터 유실 위험).


## Follower는 리더를 어떻게 따라가는가?

1. 팔로워는 리더에게 `FetchRequest`를 보낸다.
2. 예: "나 offset 123까지 받았는데, 다음 메시지 줘!"
3. 리더는 해당 offset 이후 메시지를 전달한다.
4. 팔로워는 디스크에 저장하며 리더를 계속 따라간다.

📌 10초 이상 메시지를 안 받아가거나 너무 느리면 → ISR 탈락  
→ 다음 리더 선출 시 후보에서 제외된다.


## 📖 Follower에서 읽기 기능도 지원된다

과거에는?
- Kafka는 무조건 리더에서만 read가 가능했다.
- 리더 브로커에 트래픽 집중, 지연 증가

지금은?
- **Read from Follower** 기능 도입
- 단, **조건이 있음**:
  - 팔로워도 리더만큼 신뢰할 수 있어야 함
  - **committed message만 읽을 수 있어야 함**

📌 예: 클라이언트가 일본에 있고, 리더는 미국 →  
→ 일본에 있는 팔로워에서 데이터를 읽으면 **지연 시간 감소**

### 지역 기반 읽기를 위한 설정

- 클라이언트 설정: `client.rack=jp-tokyo`
- 브로커 설정: `broker.rack=jp-tokyo`
- 레플리카 선택자 설정:  
  `replica.selector.class=org.apache.kafka.common.replica.RackAwareReplicaSelector`
